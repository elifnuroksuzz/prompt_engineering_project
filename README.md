# Prompt Engineering Framework

**GeliÅŸmiÅŸ AI model performans testi ve prompt stratejisi analiz sistemi**

## Ã–zellikler

- **Ã‡oklu Prompt Stratejileri**: Zero-shot, One-shot, Few-shot, Chain-of-Thought
- **Otomatik DeÄŸerlendirme**: SayÄ±sal doÄŸruluk, format analizi, aÃ§Ä±klama kalitesi
- **KapsamlÄ± Raporlama**: CSV, JSON, HTML formatlarÄ±nda detaylÄ± analiz
- **Real-time API Entegrasyonu**: Gemini AI modeli ile gerÃ§ek zamanlÄ± test
- **Ä°statistiksel Analiz**: Strateji karÅŸÄ±laÅŸtÄ±rmasÄ± ve performans metrikleri

## Desteklenen GÃ¶revler

### ğŸ“ Text Classification
- Duygu analizi (Sentiment Analysis)
- TÃ¼rkÃ§e metin sÄ±nÄ±flandÄ±rma
- Ã‡oklu etiket desteÄŸi

### ğŸ§® Mathematical Reasoning
- Ä°ki bilinmeyenli denklem sistemleri
- AdÄ±m adÄ±m Ã§Ã¶zÃ¼m analizi
- Chain-of-Thought deÄŸerlendirmesi

## Kurulum

```bash
git clone https://github.com/yourusername/prompt-engineering-framework
cd prompt-engineering-framework
pip install -r requirements.txt
```

## KonfigÃ¼rasyon

1. `config/.env` dosyasÄ±nÄ± oluÅŸturun:
```bash
GEMINI_API_KEY=your_api_key_here
```

2. `config/settings.yaml` dosyasÄ±nÄ± dÃ¼zenleyin:
```yaml
model:
  name: "gemini-2.5-flash"
  temperature: 0.1
  mock_mode: false  # GerÃ§ek API iÃ§in
```

## KullanÄ±m

### Temel Komutlar

```bash
# Mevcut gÃ¶revleri listele
python main.py --list-tasks

# Tek gÃ¶rev Ã§alÄ±ÅŸtÄ±r
python main.py --task mathematical_reasoning --strategies vanilla zero_shot_cot

# TÃ¼m gÃ¶revleri Ã§alÄ±ÅŸtÄ±r
python main.py --run-all

# Strateji karÅŸÄ±laÅŸtÄ±rmasÄ±
python main.py --task text_classification --compare-strategies

# Performance benchmark
python main.py --benchmark
```

### GeliÅŸmiÅŸ Ã–zellikler

```bash
# HTML raporu oluÅŸtur
python main.py --task mathematical_reasoning --output-format html

# Ã–zel konfigÃ¼rasyon
python main.py --task text_classification --config config/test_settings.yaml
```

## Ã‡Ä±ktÄ± Ã–rnekleri

### Konsol Ã‡Ä±ktÄ±sÄ±
```
==================================================
EXPERIMENT RESULTS SUMMARY
==================================================
Performance by Strategy:
  Prompt Type Prompt Format  mean  std  count
      vanilla       vanilla   1.0  0.0      3
zero_shot_cot zero_shot_cot   1.0  0.0      3

Total tests: 6
Average accuracy: 1.000
```

### Dosya Ã‡Ä±ktÄ±larÄ±
- `data/output/mathematical_reasoning_20250823_001839.csv`
- `data/output/reports/comprehensive_report_20250823_002729.html`
- `data/output/experiment_summary.json`

## Proje YapÄ±sÄ±

```
prompt_engineering_project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml          # Ana konfigÃ¼rasyon
â”‚   â”œâ”€â”€ test_settings.yaml     # Test konfigÃ¼rasyonu
â”‚   â””â”€â”€ .env                   # API anahtarlarÄ±
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # KonfigÃ¼rasyon yÃ¶netimi
â”‚   â”‚   â””â”€â”€ model_manager.py   # AI model entegrasyonu
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ text_classification.py
â”‚   â”‚   â””â”€â”€ mathematical_reasoning.py
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ prompt_library.py  # Prompt ÅŸablonlarÄ±
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ metrics.py         # DeÄŸerlendirme metrikleri
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_handler.py    # Veri iÅŸleme
â”‚       â””â”€â”€ benchmark_runner.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ output/               # Test sonuÃ§larÄ±
â”œâ”€â”€ main.py                   # Ana Ã§alÄ±ÅŸtÄ±rma dosyasÄ±
â””â”€â”€ requirements.txt
```

## Kavramlar

### Prompt Engineering Stratejileri

- **Zero-shot**: Model'e Ã¶rnek vermeden direkt gÃ¶rev
- **One-shot**: Tek Ã¶rnek ile gÃ¶rev
- **Few-shot**: BirkaÃ§ Ã¶rnek ile gÃ¶rev  
- **Chain-of-Thought (CoT)**: AdÄ±m adÄ±m dÃ¼ÅŸÃ¼nme sÃ¼reci

### DeÄŸerlendirme Metrikleri

- **Numeric Accuracy**: SayÄ±sal sonuÃ§larÄ±n doÄŸruluÄŸu
- **Format Correctness**: YanÄ±t formatÄ±nÄ±n uygunluÄŸu
- **Explanation Quality**: AÃ§Ä±klama kalitesi analizi

### Performance Analizi

- **Strategy Comparison**: Strateji bazÄ±nda performans
- **Statistical Analysis**: Ortalama, standart sapma, tutarlÄ±lÄ±k
- **Benchmark Testing**: HÄ±z ve bellek kullanÄ±mÄ±

## API Entegrasyonu

Framework ÅŸu AI modelleri destekler:
- Google Gemini (varsayÄ±lan)
- Mock mode (test iÃ§in)

## GeliÅŸtirme

### Yeni GÃ¶rev Ekleme

```python
class NewTask(BaseTask):
    def get_task_name(self) -> str:
        return "Your Task Name"
    
    def get_test_data(self) -> List[Dict[str, Any]]:
        return [{"input_text": "...", "expected_output": "..."}]
    
    def evaluate_response(self, expected: str, actual: str) -> float:
        # DeÄŸerlendirme mantÄ±ÄŸÄ±
        return score
```

### Yeni Prompt Stratejisi

```python
def _generate_prompt(self, strategy: str, data_item: Dict[str, Any]) -> str:
    if strategy == "your_strategy":
        return f"Your prompt template: {data_item['input_text']}"
```

## KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Branch'i push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## Lisans

MIT License - Detaylar iÃ§in `LICENSE` dosyasÄ±na bakÄ±n.

## Ä°letiÅŸim

- **Proje**: [GitHub Repository](https://github.com/yourusername/prompt-engineering-framework)
- **Issues**: Bug raporlarÄ± ve Ã¶zellik istekleri iÃ§in GitHub Issues kullanÄ±n

---

**Not**: Bu framework akademik araÅŸtÄ±rma ve prompt engineering optimizasyonu iÃ§in tasarlanmÄ±ÅŸtÄ±r. Production kullanÄ±mÄ± iÃ§in ek gÃ¼venlik ve Ã¶lÃ§eklendirme Ã¶nlemleri gerekebilir.
